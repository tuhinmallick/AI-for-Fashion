{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuhinmallick/AI-for-Fashion/blob/main/Fine_tune_Llama_3_1_70B_with_Two_Consumer_GPUs_Using_FSDP_and_QLoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*More details in this article: [Multi-GPU Fine-tuning for Llama 3.1 70B with FSDP and QLoRA](https://newsletter.kaitchup.com/p/multi-gpus-fine-tuning-for-llama)*\n",
        "\n",
        "\n",
        "This notebook shows how to train a 70B LLM, e.g., Llama 3.1 70B, using two GPUs. It exploits FSDP for multi-gpus training and QLoRA for parameter-efficient fine-tuning.\n",
        "\n",
        "This code runs on two 24 GB GPUs and requires at least 200 GB of CPU RAM.\n",
        "\n",
        "*Note: This code was not tested with a Jupyter notebook. You may copy the fine-tuning code into Python file and run this Python file with Accelerate.*\n",
        "\n",
        "First, we need to install:"
      ],
      "metadata": {
        "id": "A3L5I088PRr1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiarBXAHT-S1"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade bitsandbytes transformers peft accelerate datasets trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, configure Accelerate:"
      ],
      "metadata": {
        "id": "Bti-uyQpRO0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accelerate config"
      ],
      "metadata": {
        "id": "VW_4FMVNRTzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or use the following configuration file that you may copy into a file named \"config_fsdp.yaml\""
      ],
      "metadata": {
        "id": "pGOKHP4ZRVlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_environment: LOCAL_MACHINE\n",
        "debug: false\n",
        "distributed_type: FSDP\n",
        "downcast_bf16: 'no'\n",
        "fsdp_config:\n",
        "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n",
        "  fsdp_backward_prefetch: BACKWARD_PRE\n",
        "  fsdp_cpu_ram_efficient_loading: true\n",
        "  fsdp_forward_prefetch: false\n",
        "  fsdp_offload_params: true\n",
        "  fsdp_sharding_strategy: FULL_SHARD\n",
        "  fsdp_state_dict_type: SHARDED_STATE_DICT\n",
        "  fsdp_sync_module_states: true\n",
        "  fsdp_use_orig_params: false\n",
        "machine_rank: 0\n",
        "main_training_function: main\n",
        "mixed_precision: 'no'\n",
        "num_machines: 1\n",
        "num_processes: 2\n",
        "rdzv_backend: static\n",
        "same_network: true\n",
        "tpu_env: []\n",
        "tpu_use_cluster: false\n",
        "tpu_use_sudo: false\n",
        "use_cpu: false"
      ],
      "metadata": {
        "id": "OI8pGYCSRjZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Llama 3.1. Make sure you have an access token and it is loaded into your environment:"
      ],
      "metadata": {
        "id": "L5cnhP-7QO8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"hf_token\") #Replace \"hf_token\" with your token"
      ],
      "metadata": {
        "id": "dHLjbygAQtxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fine-tuning code below must be run with accelerate. Copy it into a file, e.g., \"fsdp+QLoRA.py\" and then run\n",
        "\n",
        "\n",
        "```\n",
        "accelerate launch --config_file config_fsdp.yaml fsdp+QLoRA.py\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Exb6Q9dYQ0e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, multiprocessing\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    set_seed\n",
        ")\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from peft.utils.other import fsdp_auto_wrap_policy\n",
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "set_seed(1234)\n",
        "\n",
        "#use bf16 and FlashAttention if supported\n",
        "if torch.cuda.is_bf16_supported():\n",
        "  os.system('pip install flash_attn')\n",
        "  compute_dtype = torch.bfloat16\n",
        "  attn_implementation = 'flash_attention_2'\n",
        "else:\n",
        "  compute_dtype = torch.float16\n",
        "  attn_implementation = 'sdpa'\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-70B\"\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
        "tokenizer.pad_token_id = 128004\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "ds = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
        "\n",
        "#Add the EOS token\n",
        "def process(row):\n",
        "    row[\"text\"] = row[\"text\"]+\"<|end_of_text|>\"\n",
        "    return row\n",
        "\n",
        "ds = ds.map(\n",
        "    process,\n",
        "    num_proc= multiprocessing.cpu_count(),\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_storage=compute_dtype,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_name, quantization_config=bnb_config, torch_dtype=torch.bfloat16, attn_implementation=attn_implementation\n",
        ")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    # freeze base model's layers\n",
        "    param.requires_grad = False\n",
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':True})\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        r=16,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
        ")\n",
        "\n",
        "output_dir = \"./Llama3.1_70b_QLoRA/\"\n",
        "\n",
        "\n",
        "training_arguments = SFTConfig(\n",
        "        output_dir=output_dir ,\n",
        "        eval_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        optim=\"adamw_torch\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        per_device_eval_batch_size=1,\n",
        "        log_level=\"debug\",\n",
        "        logging_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        bf16 = True,\n",
        "        eval_steps=10,\n",
        "        max_steps=50,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=512,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=ds['train'],\n",
        "        eval_dataset=ds['test'],\n",
        "        peft_config=peft_config,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments,\n",
        ")\n",
        "\n",
        "\n",
        "fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n",
        "fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "if trainer.is_fsdp_enabled:\n",
        "    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
        "\n",
        "trainer.save_model(output_dir)"
      ],
      "metadata": {
        "id": "PfunHDnXXbiS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}